{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%config InlineBackend.figure_format = \"retina\"\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from matplotlib import rcParams\n",
    "rcParams[\"savefig.dpi\"] = 100\n",
    "rcParams[\"figure.dpi\"] = 100\n",
    "rcParams[\"font.size\"] = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian optimization \n",
    "\n",
    "This notebook was made with the following version of george:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import george\n",
    "george.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we'll show a very simple example of implementing \"Bayesian optimization\" using george.\n",
    "Now's not the time to get into a discussion of the issues with the name given to these methods, but I think that the \"Bayesian\" part of the title comes from the fact that the method relies on the (prior) assumption that the objective function is smooth.\n",
    "The basic idea is that you can reduce the number of function evaluations needed to minimize a black-box function by using a GP as a surrogate model.\n",
    "This can be huge if evaluating your model is computationally expensive.\n",
    "I think that the classic reference is [Jones et al. (1998)](https://doi.org/10.1023/A:1008306431147) and the example here will look a bit like their section 4.1.\n",
    "\n",
    "First, we'll define the scalar objective that we want to minimize in the range $-5 \\le x \\le 5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib notebook\n",
    "\"\"\"\n",
    "def objective(theta):\n",
    "    return -0.5 * np.exp(-0.5*(theta - 2)**2) - 0.5 * np.exp(-0.5 * (theta + 2.1)**2 / 5) + 0.3\n",
    "\"\"\"\n",
    "def objective(X,y):\n",
    "    return (4 - 2.1*X**2 + X**4/3)*X**2 + X * Y + (-4 + 4*Y**2) * Y**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y = np.linspace(-2, 2, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = objective(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100)\n",
      "(100, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "Z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.clf()\n",
    "# fig = plt.figure()\n",
    "# ax = fig.gca(projection='3d')\n",
    "# ax.contour3D(X, Y, Z,100, cmap='viridis')\n",
    "# ax.set_xlabel('x')\n",
    "# ax.set_ylabel('y')\n",
    "# ax.set_zlabel('z')\n",
    "# plt.title(\"Six Hump\")\n",
    "# # ax.plot(searchSpace[:,0], searchSpace[:,1], obj, label='Six Hump')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for the \"Bayesian\" optimization, the basic procedure that we'll follow is:\n",
    "\n",
    "1. Start by evaluating the model at a set of points. In this case, we'll start with a uniform grid in $x$.\n",
    "2. Fit a GP (optimize the hyperparameters) to the set of training points.\n",
    "3. Find the input coordinate that maximizes the \"expected improvement\" (see Section 4 of Jones+ 1998). For simplicity, we simply use a grid search to maximize this, but this should probably be a numerical optimization in any real application of this method.\n",
    "4. At this new coordinate, run the model and add this as a new training point.\n",
    "5. Return to step 2 until converged. We'll judge convergence using relative changes in the location of the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of someone getting the GP to work with 2D data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielzeiberg/miniconda3/lib/python3.6/site-packages/scipy/optimize/_minimize.py:502: RuntimeWarning: Method Nelder-Mead does not use gradient information (jac).\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "CPU times: user 9.83 s, sys: 422 ms, total: 10.2 s\n",
      "Wall time: 5.34 s\n",
      "(1000, 2) (1000,)\n"
     ]
    }
   ],
   "source": [
    "def fake_2d_data(n_samples):\n",
    "    x = np.sqrt(n_samples / 10.0) * np.random.RandomState(12356).rand(n_samples, 2)\n",
    "    yerr = 0.2 * np.ones_like(x[:, 0])\n",
    "    y = np.sin(x[:, 0] + x[:, 1]) + yerr * np.random.randn(len(x))\n",
    "    return x, y\n",
    "\n",
    "n_samples = 1000\n",
    "x, y = fake_2d_data(n_samples)\n",
    "\n",
    "kernel = ConstantKernel(0.5, ndim=2) * Matern32Kernel(0.5, ndim=2) + WhiteKernel(0.1, ndim=2)\n",
    "gp = george.GP(kernel)\n",
    "gp.compute(x)\n",
    "\n",
    "%time gp.optimize(x, y, verbose=True, method='Nelder-Mead')\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_theta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "3-th leading minor of the array is not positive definite",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-822f0818833f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# gp = george.GP(kernel, fit_mean=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mgp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeorge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mgp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_theta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/george/gp.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, x, yerr, sort, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;31m# Set up and pre-compute the solver.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolver_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolver_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_yerr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         self._const = -0.5 * (len(self._x) * np.log(2 * np.pi)\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/george/basic.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, x, yerr)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# Factor the matrix and compute the log-determinant.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_determinant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_factor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/scipy/linalg/decomp_cholesky.py\u001b[0m in \u001b[0;36mcholesky\u001b[0;34m(a, lower, overwrite_a, check_finite)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \"\"\"\n\u001b[1;32m     90\u001b[0m     c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n\u001b[0;32m---> 91\u001b[0;31m                          check_finite=check_finite)\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/scipy/linalg/decomp_cholesky.py\u001b[0m in \u001b[0;36m_cholesky\u001b[0;34m(a, lower, overwrite_a, clean, check_finite)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n\u001b[0;32m---> 40\u001b[0;31m                           \"definite\" % info)\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         raise ValueError('LAPACK reported an illegal value in {}-th argument'\n",
      "\u001b[0;31mLinAlgError\u001b[0m: 3-th leading minor of the array is not positive definite"
     ]
    }
   ],
   "source": [
    "from george import kernels\n",
    "from george.kernels import Matern32Kernel, ConstantKernel, WhiteKernel\n",
    "from scipy.special import erf\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# N_init = 4\n",
    "# train_theta = np.linspace(-5, 5, N_init + 1)[1:]\n",
    "# train_theta -= 0.5 * (train_theta[1] - train_theta[0])\n",
    "# train_f = objective(train_theta)\n",
    "train_theta = np.array([[X.reshape(-1,1)], [Y.reshape(-1,1)]]).reshape(-1,2)\n",
    "train_f = Z.reshape(-1,1)\n",
    "kernel = ConstantKernel(0.5, ndim=2) * Matern32Kernel(0.5, ndim=2) + WhiteKernel(0.1, ndim=2)\n",
    "# gp = george.GP(kernel, fit_mean=True)\n",
    "gp = george.GP(kernel)\n",
    "gp.compute(train_theta)\n",
    "\n",
    "def nll(params):\n",
    "    gp.set_parameter_vector(params)\n",
    "    g = gp.grad_log_likelihood(train_f, quiet=True)\n",
    "    return -gp.log_likelihood(train_f, quiet=True), -g\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(8, 6))\n",
    "j = 0\n",
    "old_min = None\n",
    "converged = False\n",
    "\n",
    "for i in range(1000):\n",
    "    # Update the GP parameters\n",
    "    soln = minimize(nll, gp.get_parameter_vector(), jac=True)\n",
    "\n",
    "    # Compute the acquisition function\n",
    "    mu, var = gp.predict(train_f, t, return_var=True)\n",
    "    std = np.sqrt(var)    \n",
    "    f_min = np.min(train_f)\n",
    "    chi = (f_min - mu) / std \n",
    "    Phi = 0.5 * (1.0 + erf(chi / np.sqrt(2)))\n",
    "    phi = np.exp(-0.5 * chi**2) / np.sqrt(2*np.pi*var)\n",
    "    A_ei = (f_min - mu) * Phi + var * phi\n",
    "    A_max = t[np.argmax(A_ei)]\n",
    "\n",
    "    # Add a new point\n",
    "    train_theta = np.append(train_theta, A_max)\n",
    "    train_f = np.append(train_f, objective(train_theta[-1]))\n",
    "    gp.compute(train_theta)\n",
    "    \n",
    "    # Estimate the minimum - I'm sure that there's a better way!\n",
    "    i_min = np.argmin(mu)\n",
    "    sl = slice(max(0, i_min - 1), min(len(t), i_min + 2))\n",
    "    ts = t[sl]\n",
    "    D = np.vander(np.arange(len(ts)).astype(float))\n",
    "    w = np.linalg.solve(D, mu[sl])\n",
    "    minimum = ts[0] + (ts[1] - ts[0]) * np.roots(np.polyder(w[::-1]))\n",
    "    \n",
    "    # Check convergence\n",
    "    if i > 0 and np.abs((old_min - minimum) / minimum) < 1e-5:\n",
    "        converged = True\n",
    "    old_min = float(minimum[0])\n",
    "    \n",
    "    # Make the plots\n",
    "    if converged or i in [0, 1, 2]:\n",
    "        ax = axes.flat[j]\n",
    "        j += 1\n",
    "        ax.plot(t, objective(t))\n",
    "        ax.plot(t, mu, \"k\")\n",
    "        ax.plot(train_theta[:-1], train_f[:-1], \"or\")\n",
    "        ax.plot(train_theta[-1], train_f[-1], \"og\")\n",
    "        ax.fill_between(t, mu+std, mu-std, color=\"k\", alpha=0.1)\n",
    "        if i <= 3:\n",
    "            ax2 = ax.twinx()\n",
    "            ax2.plot(t, A_ei, \"g\", lw=0.75)\n",
    "            ax2.set_yticks([])\n",
    "        ax.axvline(old_min, color=\"k\", lw=0.75)\n",
    "        ax.set_ylim(-0.37, 0.37)\n",
    "        ax.set_xlim(-5, 5)\n",
    "        ax.set_yticklabels([])\n",
    "        ax.annotate(\"step {0}; {1:.3f}\".format(i+1, old_min), xy=(0, 1),\n",
    "                    xycoords=\"axes fraction\", ha=\"left\", va=\"top\",\n",
    "                    xytext=(5, -5), textcoords=\"offset points\",\n",
    "                    fontsize=14)\n",
    "    \n",
    "    if converged:\n",
    "        break\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "print(\"{0} model evaluations\".format(len(train_f)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There's a lot going on in these plots.\n",
    "Each panel shows the results after a certain iteration (indicated in the top left corner of the panel).\n",
    "In each panel:\n",
    "\n",
    "1. The blue line is the true objective function.\n",
    "2. The black line and gray contours indicate the current estimate of the objective using the GP model.\n",
    "3. The green line is the expected improvement.\n",
    "4. The red points are the training set.\n",
    "5. The green point is the new point that was added at this step.\n",
    "6. The vertical black line is the current estimate of the location minimum. This is also indicated in the top left corner of the panel.\n",
    "\n",
    "As you can see, only 10 model evaluations (including the original training set) were needed to converge to the correct minimum.\n",
    "In this simple example, there are certainly other methods that could have easily been used to minimize this function, but you can imagine that this method could be useful for cases where `objective` is very expensive to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
