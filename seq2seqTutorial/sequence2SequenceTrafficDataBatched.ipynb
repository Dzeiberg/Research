{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "clip = 50\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data():\n",
    "    def __init__(self, filepath):\n",
    "        data = np.load(filepath)\n",
    "        xs = data['x'][:,:,:,0]\n",
    "        ys = data['y'][:,:,:,0]\n",
    "        self.xExamples = xs.reshape((data['x'].shape[0], data['x'].shape[1], -1)) # num_examples x seq_length x num_features\n",
    "        self.yExamples = ys.reshape((data['y'].shape[0], data['y'].shape[1], -1)) # num_examples x seq_length x num_features\n",
    "        \n",
    "    def getSequenceLength(self):\n",
    "        return self.xExamples.shape[0]\n",
    "    \n",
    "    def getNumFeatures(self):\n",
    "        return sef.xExamples.shape[1]\n",
    "\n",
    "    def random_batch(self, batch_size):\n",
    "        input_seqs = []\n",
    "        target_seqs = []\n",
    "        \n",
    "        #Choose random pairs\n",
    "        for i in range(batch_size):\n",
    "            pairIDX = np.random.randint(0, self.xExamples.shape[0])\n",
    "            input_seqs.append(self.xExamples[pairIDX, :, :])\n",
    "            target_seqs.append(self.yExamples[pairIDX, :, :])\n",
    "            \n",
    "        input_lengths = torch.IntTensor([len(s) for s in input_seqs])\n",
    "        target_lengths = torch.IntTensor([len(s) for s in target_seqs])\n",
    "        \n",
    "        #convert to tensors, transpose into (max_len, x batch_size)\n",
    "        \n",
    "        #input_var = Variable(torch.LongTensor(input_padded)).transpose(0, 1)\n",
    "        #target_var = Variable(torch.LongTensor(target_padded)).transpose(0, 1)\n",
    "        inputTensor = torch.FloatTensor(input_seqs).transpose(0, 1)\n",
    "        targetTensor = torch.FloatTensor(target_seqs).transpose(0, 1)\n",
    "        return inputTensor, input_lengths, targetTensor, target_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrafficDataTrainObj = Data(\"../DCRNN/data/train.npz\")\n",
    "TrafficDataValObj = Data(\"../DCRNN/data/val.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = TrafficDataTrainObj.xExamples.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 2, 207])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputTTest, input_lengthsTest, targetTTest, target_lengthsTest = TrafficDataTrainObj.random_batch(2)\n",
    "inputTTest.size()\n",
    "#TrafficDataTrainObj.xExamples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, sequence_length, num_features, hidden_size, n_layers=1, dropout=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_features = num_features\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.gru = nn.GRU(num_features, hidden_size, n_layers, dropout=self.dropout)\n",
    "        \n",
    "    def forward(self, input_seqs, input_lengths, hidden=None):\n",
    "        # Note: we run this all at once (over multiple batches of multiple sequences)\n",
    "        #print(\"encoder input size\", input_seqs.size())\n",
    "        sequence_lengths = torch.IntTensor([self.sequence_length for i in range(input_seqs.size(1))])\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(input_seqs, sequence_lengths)\n",
    "        #print(\"packed size\",packed.data.size())\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        #print(\"encoder output size\", outputs.size())\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(1, self.hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        #hidden (rnn_output) size  torch.Size([1, 3, 8])\n",
    "        #encoder_outputs size  torch.Size([12, 3, 8])\n",
    "        max_len = encoder_outputs.size(0)\n",
    "        this_batch_size = encoder_outputs.size(1)\n",
    "        # Create variable to store attention energies\n",
    "        attn_energies = torch.autograd.Variable(torch.zeros(this_batch_size, max_len)) # B x S\n",
    "        #if USE_CUDA:\n",
    "        #    attn_energies = attn_energies.cuda()\n",
    "\n",
    "        # For each batch of encoder outputs\n",
    "        for b in range(this_batch_size):\n",
    "            # Calculate energy for each encoder output\n",
    "            for i in range(max_len):\n",
    "                # changing order of :,b in hidden[:,b]\n",
    "                attn_energies[b, i] = self.score(hidden[:,b], encoder_outputs[i, b].unsqueeze(0))\n",
    "\n",
    "        # Normalize energies to weights in range 0 to 1, resize to 1 x B x S\n",
    "        return F.softmax(attn_energies).unsqueeze(1)\n",
    "    \n",
    "    def score(self, hidden, encoder_output):\n",
    "        if self.method == \"general\":\n",
    "            energy = self.attn(encoder_output)\n",
    "            energy = energy.squeeze()\n",
    "            hidden = hidden.squeeze()\n",
    "            energy = hidden.dot(energy)\n",
    "\n",
    "            return energy\n",
    "        else:\n",
    "            assert False, \"sorry I didn't implement that method yet\"\n",
    "            \"\"\"\n",
    "            if self.method == 'dot':\n",
    "                energy = hidden.dot(encoder_output)\n",
    "                return energy\n",
    "\n",
    "            elif self.method == 'general':\n",
    "                energy = self.attn(encoder_output)\n",
    "                energy = energy.squeeze()\n",
    "                hidden = hidden.squeeze()\n",
    "                energy = hidden.dot(energy)\n",
    "\n",
    "                return energy\n",
    "\n",
    "            elif self.method == 'concat':\n",
    "                energy = self.attn(torch.cat((hidden, encoder_output), 1))\n",
    "                energy = self.v.dot(energy)\n",
    "                return energy\n",
    "            \"\"\"                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout=0.1, method=None):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        # Define layers\n",
    "        self.gru = nn.GRU(self.output_size, hidden_size, n_layers, dropout=dropout)\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        if method != None:\n",
    "            self.attn = Attn(method, hidden_size)\n",
    "\n",
    "    def forward(self, input_seq, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step at a time S = 1\n",
    "        batch_size = input_seq.size(0)\n",
    "        expandedInput = input_seq.unsqueeze(0)# S=1 x B X output size\n",
    "        \n",
    "        # Get current hidden state from decoder input and last hidden state\n",
    "        rnn_output, hidden = self.gru(expandedInput, last_hidden)\n",
    "        \n",
    "        #Calculate attention from current RNN state and all encoder outputs\n",
    "        #apply  to encoder outputs to get weighted attention\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0,1)) #B x S x NHidden\n",
    "        \n",
    "        rnn_output = rnn_output.squeeze(0) # S=1 x B x NHidden -> B x NHidden\n",
    "        context = context.squeeze(1) # B x S=1 x NHidden -> B x NHidden\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = F.tanh(self.concat(concat_input))\n",
    "        \n",
    "        output = self.out(concat_output)\n",
    "        \n",
    "        # Return final output, hidden state\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        # Define layers\n",
    "        self.gru = nn.GRU(self.output_size, hidden_size, n_layers, dropout=dropout)\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq, last_hidden):\n",
    "        # Note: we run this one step at a time S = 1\n",
    "        batch_size = input_seq.size(0)\n",
    "        expandedInput = input_seq.unsqueeze(0)# S=1 x B X output size\n",
    "        \n",
    "        # Get current hidden state from decoder input and last hidden state\n",
    "        rnn_output, hidden = self.gru(expandedInput, last_hidden)\n",
    "        \n",
    "        output = self.out(rnn_output)\n",
    "        \n",
    "        # Return final output, hidden state\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModels():\n",
    "    small_batch_size = 3\n",
    "    input_batches, input_lengths, target_batches, target_lengths = TrafficDataTrainObj.random_batch(small_batch_size)\n",
    "    print('input_batches', input_batches.size()) # (max_len x batch_size x NFeatues)\n",
    "    print('target_batches', target_batches.size()) # (max_len x batch_size x NFeatures)\n",
    "    small_hidden_size = 8\n",
    "    small_n_layers = 2\n",
    "    \n",
    "    #sequence_lengths, num_features, hidden_size, n_layers=1, dropout=0.1\n",
    "    encoderTest = EncoderRNN(input_lengths[0], input_batches.size(2), small_hidden_size, n_layers=small_n_layers)\n",
    "    \n",
    "    #attn_model, hidden_size, output_size, n_layers=1, dropout=0.1\n",
    "    #decoderTest = LuongAttnDecoderRNN('general', small_hidden_size, target_batches.size(2), n_layers=small_n_layers)\n",
    "    \n",
    "    #hidden_size, output_size, n_layers=1, dropout=0.1\n",
    "    decoderTest = DecoderRNN(small_hidden_size, target_batches.size(2), n_layers=small_n_layers)\n",
    "    \n",
    "    #Encode\n",
    "    encoder_outputs, encoder_hidden = encoderTest(input_batches, input_lengths, hidden=None)\n",
    "    print('encoder_outputs', encoder_outputs.size()) # max_len x batch_size x hidden_size\n",
    "    print('encoder_hidden', encoder_hidden.size()) # n_layers x batch_size x hidden_size\n",
    "    \n",
    "    # Decoder\n",
    "    max_target_length = max(target_lengths)\n",
    "    decoder_input = torch.autograd.Variable(torch.FloatTensor(np.zeros((small_batch_size, target_batches.size(2)))))\n",
    "\n",
    "    # use last encoder hidden as initial decoder hidden\n",
    "    decoder_hidden = encoder_hidden\n",
    "    all_decoder_outputs = torch.autograd.Variable(torch.zeros(max_target_length, small_batch_size, target_batches.size(2)))\n",
    "\n",
    "    # Run through decoder one time step at a time\n",
    "    for t in range(max_target_length):\n",
    "        #print(\"t=\",t)\n",
    "        decoder_output, decoder_hidden = decoderTest(decoder_input, decoder_hidden)\n",
    "        all_decoder_outputs[t] = decoder_output\n",
    "        decoder_input = target_batches[t] # Next input is current target\n",
    "        \n",
    "    loss = nn.L1Loss()\n",
    "    lossVal = loss(all_decoder_outputs.detach(), target_batches)\n",
    "    print(\"loss\", lossVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_batches torch.Size([12, 3, 207])\n",
      "target_batches torch.Size([12, 3, 207])\n",
      "encoder_outputs torch.Size([12, 3, 8])\n",
      "encoder_hidden torch.Size([2, 3, 8])\n",
      "loss tensor(57.5563)\n"
     ]
    }
   ],
   "source": [
    "testModels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModelAttn():\n",
    "    small_batch_size = 3\n",
    "    input_batches, input_lengths, target_batches, target_lengths = TrafficDataTrainObj.random_batch(small_batch_size)\n",
    "    print('input_batches', input_batches.size()) # (max_len x batch_size x NFeatues)\n",
    "    print('target_batches', target_batches.size()) # (max_len x batch_size x NFeatures)\n",
    "    small_hidden_size = 8\n",
    "    small_n_layers = 2\n",
    "    \n",
    "    #sequence_lengths, num_features, hidden_size, n_layers=1, dropout=0.1\n",
    "    encoderTest = EncoderRNN(input_lengths, input_batches.size(2), small_hidden_size, n_layers=small_n_layers)\n",
    "    \n",
    "    #self, hidden_size, output_size, n_layers=1, dropout=0.1\n",
    "    decoderTest = AttnDecoderRNN(small_hidden_size, target_batches.size(2), n_layers=small_n_layers, method=\"general\")\n",
    "    \n",
    "    #Encode\n",
    "    encoder_outputs, encoder_hidden = encoderTest(input_batches, input_lengths, None)\n",
    "    print('encoder_outputs', encoder_outputs.size()) # max_len x batch_size x hidden_size\n",
    "    print('encoder_hidden', encoder_hidden.size()) # n_layers x batch_size x hidden_size\n",
    "    \n",
    "    # Decoder\n",
    "    max_target_length = max(target_lengths)\n",
    "    decoder_input = torch.autograd.Variable(torch.FloatTensor(np.zeros((small_batch_size, target_batches.size(2)))))\n",
    "\n",
    "    # use last encoder hidden as initial decoder hidden\n",
    "    decoder_hidden = encoder_hidden\n",
    "    all_decoder_outputs = torch.autograd.Variable(torch.zeros(max_target_length, small_batch_size, target_batches.size(2)))\n",
    "\n",
    "    # Run through decoder one time step at a time\n",
    "    for t in range(max_target_length):\n",
    "        #print(\"t=\",t)\n",
    "        decoder_output, decoder_hidden, attnVec = decoderTest(decoder_input, decoder_hidden, encoder_outputs)\n",
    "        all_decoder_outputs[t] = decoder_output\n",
    "        decoder_input = target_batches[t] # Next input is current target\n",
    "        \n",
    "    loss = nn.L1Loss()\n",
    "    lossVal = loss(all_decoder_outputs.detach(), target_batches)\n",
    "    print(\"loss\", lossVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_batches torch.Size([12, 3, 207])\n",
      "target_batches torch.Size([12, 3, 207])\n",
      "encoder_outputs torch.Size([12, 3, 8])\n",
      "encoder_hidden torch.Size([2, 3, 8])\n",
      "loss tensor(58.7094)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielzeiberg/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/Users/danielzeiberg/miniconda3/lib/python3.6/site-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "testModelAttn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainOneBatch(input_batch, input_lengths, target_batch, target_lengths, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    \n",
    "    # Zero gradients of both optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0 # Added onto for each word\n",
    "\n",
    "    # Run words through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_batches, input_lengths, None)\n",
    "    \n",
    "    # Prepare decoder input and output variables\n",
    "    decoder_input = torch.autograd.Variable(torch.FloatTensor(np.zeros((small_batch_size, target_batches.size(2)))))\n",
    "    \n",
    "    # use last encoder hidden as initial decoder hidden\n",
    "    decoder_hidden = encoder_hidden\n",
    "    all_decoder_outputs = torch.autograd.Variable(torch.zeros(max(target_lengths), small_batch_size, target_batches.size(2)))\n",
    "\n",
    "    # Decode\n",
    "    for t in range(max(target_lengths\n",
    "                      )):\n",
    "        decoder_output, decoder_hidden, decoder_attn = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs\n",
    "        )\n",
    "        all_decoder_outputs[t] = decoder_output\n",
    "        decoder_input = target_batches[t] # Next input is current target\n",
    "    \n",
    "    #Calculate Loss\n",
    "    if criterion[1] == \"MSE\":\n",
    "        loss += torch.sqrt(criterion[0])(all_decoder_outputs.detach(), target_batch)\n",
    "    elif criterion[1] == \"Mean Absolute Error\":\n",
    "        loss += criterion[0](all_decoder_outputs.detach(), target_batch)\n",
    "    else:\n",
    "        assert False, \"Cannot match loss\"\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item() / max(target_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataObj, n_layers= 2, hidden_state_size=64,n_epochs=100, batch_size = 64, initialLR = 1e-2, lrDecayRatio=0.10, lrDecayBegginingEpoch=20, lrDecayEvery=10):\n",
    "    # Encoder Params:\n",
    "    # sequence_lengths, num_features, hidden_size, n_layers=1, dropout=0.1)\n",
    "    \n",
    "    sequence_length = dataObj.getSequenceLength()\n",
    "    num_features = dataObj.getNumFeatures()\n",
    "    \n",
    "    encoderTest = EncoderRNN(sequence_length, num_features,\n",
    "                             hidden_state_size, n_layers=n_layers)\n",
    "    \n",
    "    decoderTest = AttnDecoderRNN(hidden_state_size, num_features,\n",
    "                                 n_layers=n_layers, method=\"general\")\n",
    "    lr = initialLR\n",
    "    for epoch in range(n_epochs):\n",
    "        # Check whether we need to reduce learning rate\n",
    "        if (epoch >= 20) and (epoch % 10 == 0):\n",
    "            lr = lr * (1 - lrDecayRatio)\n",
    "        encoder_optimizer = optim.SGD(encoder.parameters(), lr=lr)\n",
    "        decoder_optimizer = optim.SGD(decoder.parameters(), lr=lr)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, xMatrix, yMatrix, n_iters, model_description, xVal, yVal, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    validationLosses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    #criterion = (nn.MSELoss(),\"MSE\")\n",
    "    criterion = (nn.L1Loss(size_average=True), \"Mean Absolute Error\")\n",
    "    #valLoss = (nn.MSELoss(), \"MSE\")\n",
    "    valLoss = (nn.L1Loss(size_average=True), \"Mean Absolute Error\")\n",
    "    for iter in range(1, n_iters+1):\n",
    "        choice = np.random.randint(0, xMatrix.shape[0])\n",
    "        input_tensor = torch.FloatTensor(xMatrix[choice], device=device)\n",
    "        target_tensor = torch.FloatTensor(yMatrix[choice], device=device)\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        \n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "            # Get validation loss\n",
    "            validationLosses.append(validate(encoder, decoder, xVal, yVal, valLoss[0]))\n",
    "    showPlot(validationLosses, plot_every, model_description, criterion[1], training=False)\n",
    "    showPlot(plot_losses, plot_every, model_description, valLoss[1], training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def showPlot(points, plot_every, model_description, lossDescription, training=True):\n",
    "    plt.rcParams.update({'font.size': 8})\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    #loc = ticker.MultipleLocator(base=0.2)\n",
    "    #ax.yaxis.set_major_locator(loc)\n",
    "    \n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.ylabel(lossDescription)\n",
    "    plt.plot(np.arange(len(points))*plot_every,points)\n",
    "    plt.grid()\n",
    "    plt.title(\"{} {} - {}\".format(\"training\" if training else \"Validation\", lossDescription, model_description))\n",
    "    filestring = \"./figs/{}_loss_plot_0.png\".format(\"training\" if training else \"Validation\")\n",
    "    while(os.path.isfile(filestring)):\n",
    "        filestring = filestring[:-5] + str(int(filestring[-5]) + 1) + \".png\"\n",
    "    plt.savefig(filestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, inputSequence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = inputSequence\n",
    "        input_length = input_tensor.size()[0] # should be 12 for traffic data 1 hr.\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        \n",
    "        encoder_outputs = torch.zeros(\n",
    "            max_length,encoder.hidden_size, device=device)\n",
    "        \n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "        \n",
    "        decoder_input = torch.tensor([np.zeros(input_tensor.size()[-1])], device=device)\n",
    "        \n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        nextSequence = []\n",
    "        #decoder_attention = torch.zeros(max_length, max_length)\n",
    "        \n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            \n",
    "            #decoder_attentions[di] = decoder_attention.data\n",
    "            #topv, topi = decoder_output.data.topk(1)\n",
    "#             if topi.item() == NULLCHAR:\n",
    "#                 nextSequence.append('XX')\n",
    "#                 break\n",
    "#             else:\n",
    "#                 nextSequence.append(topi.item())\n",
    "            nextSequence.append(decoder_output.data.squeeze().detach())\n",
    "            \n",
    "            decoder_input = decoder_output.data.squeeze().detach()\n",
    "        return nextSequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(tensorPairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        nextSequence = evaluate(encoder, decoder, pair[0])\n",
    "        outputSequence = ' '.join(nextSequence)\n",
    "        print('<', outputSequence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(encoder, decoder, xMatrix, yMatrix, lossFn, n_vals=100):\n",
    "    loss = 0.0\n",
    "    for i in range(n_vals):\n",
    "        choice = np.random.randint(0, xMatrix.shape[0])\n",
    "        input_tensor = torch.FloatTensor(xMatrix[choice], device=device)\n",
    "        target_tensor = torch.FloatTensor(yMatrix[choice], device=device)\n",
    "        nextSequence = evaluate(encoder, decoder, input_tensor)\n",
    "        l = 0.0\n",
    "        for ps, ts in zip(nextSequence, target_tensor):\n",
    "            l += lossFn(ps, ts)\n",
    "        loss += l / len(nextSequence)\n",
    "    return loss / n_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "input_size = trafficX.shape[2]\n",
    "output_size = trafficY.shape[2]\n",
    "num_layers = 2\n",
    "modelDescription = \"RNN with GRU, {} unit hidden state, {} layer GRU\".format(hidden_size, num_layers) \n",
    "N_iters = 2000 #N_iters = trafficX.shape[0]\n",
    "encoder1 = EncoderRNN(input_size, hidden_size, num_layers).to(device)\n",
    "decoder1 = DecoderRNN(hidden_size, output_size, num_layers).to(device)\n",
    "trainIters(encoder1, decoder1, trafficX, trafficY, N_iters, modelDescription, trafficValX, trafficValY, print_every=100, plot_every=N_iters/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trafficData = np.load(\"../DCRNN/data/train.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trafficData.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trafficData[\"x_offsets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trafficData['x'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = nn.L1Loss(size_average=False)\n",
    "t1 = torch.FloatTensor([[1,2,3], [4,5,6]])\n",
    "t2 = torch.FloatTensor([[2,3,4],[5,6,7]]) \n",
    "l(t1,t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate(encoder1, decoder1, trafficValX, trafficValY, nn.L1Loss(), n_vals=trafficValX.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.FloatTensor([[[1, -1],[2, -2],[3, -3],[4,-4]], [[5, -5],[6, -6],[7,-7],[8, -8]], [[9,-9],[10,-10], [11,-11],[12,-12]]])\n",
    "t1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = t1.transpose(0,1)\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
