{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        #embedded = self.embedding(input.long())\n",
    "        #embedded = embedded.view(1, 1, -1)\n",
    "        output = input.view(1,1,-1)\n",
    "        # do not need embedding because I'm using sine wave dataset\n",
    "        #output = input\n",
    "        output, hidden = self.gru(output.float(), hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        #embedded = self.embedding(input.long())\n",
    "        #embedded = embedded.view(1, 1, -1)\n",
    "        #embedded = self.dropout(embedded)\n",
    "        embedded = input.view(1,1,-1)\n",
    "        attnIn = torch.cat((embedded[0].long(), hidden[0].long()), 1)\n",
    "        attnOut = self.attn(attnIn)\n",
    "        attn_weights = F.softmax(attnOut, dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x102be1048>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3XuQXOV55/HvoxvGClkkM6LwAJYsT7BlZKTshMuqKuUYZITJovEFA4ZE3sVQqY3XcdiwiIWyMUGxHFxANmGzC/iCF5W5GQ/ygoNlDOsqjFgGSyAuwRKyAxIsGlvgJUAQoGf/6NOmp9X9nu4+p/vcfp+qruk+l+63e97u570fc3dERETqpmWdABERyRcFBhERmUKBQUREplBgEBGRKRQYRERkCgUGERGZQoFBRESmUGAQEZEpFBhERGSKGVknoBcHHXSQz58/P+tkiIgUykMPPfRLdx+KO66QgWH+/PlMTExknQwRkUIxs3/q5Dg1JYmIyBQKDCIiMoUCg4iITKHAICIiUygwiIjIFAoMIiIyRSqBwcy+bma7zOzRNvvNzP6rmW0zs0fM7Hcb9q0ys63RbVUa6RERkd6lNY/hm8DfAd9qs/8kYCS6HQP8PXCMmc0FvgiMAg48ZGbr3f2FlNKVS+ObdnLhbY/w6ut7g8eddezhXDa2eECpkjI589r7ue+p3cFjli2cy7pzjhtQiqRIUqkxuPuPgVAuXAl8y2s2Agea2SHAicAGd98dBYMNwIo00pRXy6+4l8/ftDk2KADcsPFp5q++g4vHtwwgZVIGF49vYf7qO2KDAsB9T+1W/pKWBtXHMAw80/B4R7St3fbSGd+0k/mr72Drrpe7PveGjU+z/Ip700+UlMoxazZww8anuz7vho1Pc8yaDX1IkRTVoAKDtdjmge37PoHZuWY2YWYTk5OTqSau38Y37eTzN21O9Bxbd72s4CBtHbNmA8+/tKfn859/aY+Cg/zGoALDDuCwhseHAs8Gtu/D3a9x91F3Hx0ail0DKlf+4paHU3merbte5sxr70/luaQ8ll9xb6KgUPf8S3tU+BBgcIFhPfDH0eikY4Ffu/tzwF3Ah81sjpnNAT4cbSuN5Vfcyxt7W1aCenLfU7vVJiy/sfyKe3tqnmxHNVOB9Iarfhu4HzjCzHaY2dlm9idm9ifRIXcC24FtwLXAfwBw993AXwIPRrdLo22lcOa196f6pa27YePTjG/amfrzSrFcPL6lL/lr666XVfioOHNPrzQ7KKOjo573Zbc77VeYZnDFJ5cwtrTW597JMEOAmdNg61+dnDidUlzzV9/R0XGNw54vHt/ScQf1L9Yqf5WNmT3k7qNxx2nmc59c8J1HYo8ZmTeb7V8++TdBAWDdOcdx1WlLYs99fS8q1VVYJ31NBx8wi1+sPXnKXJjLxhbzi7Unc/ABs1J5DSknBYY+uHh8C6+9EZ6nMDJvNhvO+2DLfWNLhzsKDr0MTZTiu3h8S2ytcmTebB64aHnb/Q9ctJyRebODz3HfU7vVZFlRCgwpG9+0M/YHOxQU6saWDnPWsYfHvp5KddXSSf6aBrH5i+iYGdNajRh/y/m3JBtmLcWkwJCyi74b37zTyZcWatX+uOCgUl21fOl7j8Uec0UHtc26r556VHD/63tR/qogBYYUjW/ayct73gwe00ktoNFlY4vZb0b433ThbfH9GVIOL7zyenD/soVzp/RZxRlbOsyyhXODxyh/VY8CQ4riSnMzp9HTonhf+fgHgvtffX2vSnUVEPc/njmNnhbFiztH+at6FBhSFFeau/zUzqv4jVSqE4gf6dZr/oL4mqz6GqpFgSElcZ3AZx17eFdV/Gbrzjku2KSkUl25xY1023/mtET5K67JUsOjq0WBIQXjm3bGDh9M47oKcU1KqjWU17qYkUhf/lg4b3QiLn9peHR1KDCkIK5v4cD9Z6byOnElQtUayml8087WSw5HktYW6saWDjN71vTYtEj5KTCkIK5v4ZJT3p/aa815ezjIqNZQPnFDoNOoLdSt+Wi4Zqv8VQ0KDAnFlaC6HT4Y54v/NhxkVGsol7gh0DOnxdckuzG2dFh9WaLAkFRcaS7ta+p2MiP6kvXxk6CkGOLyV5KRSO2oL0sUGBKIK82l1bfQ7LKxxcG24BdfDTdtSTHE5a+0+haaxfU1qNZQfgoMCcSV5tLsW2gW1xasL27xxQ1qSLNvoVlc/lKttNzSulDPCjN70sy2mdnqFvuvNLPN0e1nZvZiw743G/atTyM9g5BVaa4u7rn1xS2+0KCGQeQv1UqrK3FgMLPpwNXAScAi4AwzW9R4jLv/ubsvcfclwN8CtzXsfrW+z91PSZqeQbn8rieD+/tZmqsLjVDSF7fcBpG/4moNUl5p1BiOBra5+3Z33wPcCKwMHH8G8O0UXjdTO198te2+fpfm6uJGKKk5qbji/neDyF9xr6H8VV5pBIZh4JmGxzuibfsws3cBC4AfNWx+m5lNmNlGMxtLIT0DEVrFfhClOYiv7qs5qbhC/Vf9GtTQSqhWqtFJ5ZVGYGj1G9luoubpwK3u3tg4f3h0DdJPAVeZ2cKWL2J2bhRAJiYnJ5OlOKG4maiDKM3Vhar7ak4qprj+q34OamgWqpVqdFJ5pREYdgCHNTw+FHi2zbGn09SM5O7PRn+3A/cCS1ud6O7XuPuou48ODQ0lTXMinVyMZ1BU3S+fuPw1yIJH3Gup1lBOaQSGB4ERM1tgZrOo/fjvM7rIzI4A5gD3N2ybY2b7RfcPApYBj6eQpr7Jau5Cr/TFLZY85q9Qc5JqDeWUODC4+xvAZ4G7gCeAm939MTO71MwaRxmdAdzo7o2tMO8DJszsYeAeYK275zowxI0tH2Q1v05f3PLIY/6KG+SgvqzySWUeg7vf6e6/4+4L3X1NtO0L7r6+4ZhL3H1103k/cffF7n5U9PdraaSnn7IcW95O3Bc3bmit5Ece85fmNFSPZj53Ia7kPajRSM3ivrihobVSHFnlL9CchqpRYOhCqJqfVWmuTktkFF8e5i6EXjs0RFv5q1wUGLoQquZnWZoDLZFRBnka7dZKaIi2BjmUiwJDh/JcmqsbPnD/tvvUDpxvcaORQv/bQQmlQYMcykWBoUNF6MA9/8Qjsk6C9Cguf+XhfxuXhiJ8R6QzCgwdCnXg5mXugia7FVce1t6Ko0EO1aHA0IG4H9Qsxpa3o7Vtiievo91a0eikalBg6EDcpKM8lObqtLZN8RQpf6lWWg0KDB0IjUbKQ6dgo7gvrtqB86dI+SuO8lc5KDDEiCsB5aFTsFmoOUntwMWi/CVZUGCIkedJbe3oAj7FUYRh0M2Uv8pPgSFGnie1taPmpOIo4v9C+av8FBgSyGNpri7UNq3qfn6E/hd57l9Q/io3BYaAUJU4tG5MHuSxbVqmKmL/VV1c2tScVGwKDAGh/oXQujF5oGGF+VekYarNtDZXuSkwBJRpGGEztQNnr+j5S2tzlVcqgcHMVpjZk2a2zcxWt9j/aTObNLPN0e0zDftWmdnW6LYqjfSkocjV/DoNKyyuIuSvIqRRepM4MJjZdOBq4CRgEXCGmS1qcehN7r4kul0XnTsX+CJwDHA08EUzm5M0TWko4jDVZhpWmF9FHKbaTNdoKK80agxHA9vcfbu77wFuBFZ2eO6JwAZ33+3uLwAbgBUppCmxIg5TbaZhhflVls8+1NdWlvdYRWkEhmHgmYbHO6JtzT5uZo+Y2a1mdliX52Jm55rZhJlNTE5OppDs3hWhNFenYYX5VNRhqs2Uv8opjcDQqjbZXJD4HjDf3T8A/BC4votzaxvdr3H3UXcfHRoa6jmxnSjyMNVmGlaYP2Xov6pT/iqnNALDDuCwhseHAs82HuDuv3L316KH1wL/utNzsxCqAud9mGozNSflT9xnXqQaqfJXOaURGB4ERsxsgZnNAk4H1jceYGaHNDw8BXgiun8X8GEzmxN1On842papslTz61TdzxflL8m7xIHB3d8APkvtB/0J4GZ3f8zMLjWzU6LDPmdmj5nZw8DngE9H5+4G/pJacHkQuDTalpkyVfPrQmkuWtNY0VUtf4Gak4oolXkM7n6nu/+Ouy909zXRti+4+/ro/oXu/n53P8rd/8Dd/7Hh3K+7+3ui2zfSSE8SZarm14XSXLSmsaKrWv4CNScVkWY+NylbNb8TKtENTlnzl5qTykWBoUEZq/l1oVnQWtdmcKYF2u6KnL/UXFkuCgwNyljNrwvNgta6NoOzN9B2V+T8pebKclFgaFDWaj5otdU8qPJnXOX3XkQKDA2mW/tKb5Gr+XWh5iR1EPZf6DM+cP/2/5uiUP4qDwWGBm96+0pvkav5daHmJHUQ9l/oM77klPCCh0Wg/FUeCgyRUFU3VJMoklBwK8c7zK+4ZVbKUPAYWzoc7FxXc1JxKDBEQlXdUE2iLMr/DrNVpmVWQkKd62pOKg4FhkiZO547pRJd/1Qlf2k+QzkoMFDu+QvNNJ9h8KqUvzSfoRwUGCj3/IVmms8weFXKX5rPUA4KDFSnmg+az5CFKuWvOMpfxaDAQHmXKWhH480Hq+zzY5opfxVf5QPD+KadpV2moB2NNx+sss+Paab8VXyVDwxf+l77DteyVvM13nxwqjA/ppnyV/GlEhjMbIWZPWlm28xsdYv955nZ42b2iJndbWbvatj3ppltjm7rm8/ttxdead/hWsZqfp3Gmw9GVefHKH8VW+LAYGbTgauBk4BFwBlmtqjpsE3AqLt/ALgV+OuGfa+6+5Lodgo5UsZqfp3Gmw9GVTuelb+KLY0aw9HANnff7u57gBuBlY0HuPs97v5K9HAjcGgKr5tY3DIFZabx5v1XpfkLzXS5z2JLIzAMA880PN4RbWvnbOD7DY/fZmYTZrbRzMZSSE/HqrJMQSsab95/VZq/0EyX+yy2NAJDqwJmy98WMzsLGAUub9h8uLuPAp8CrjKzhW3OPTcKIBOTk5NJ0wxUt5rfCZXokqt6/lJzUnGlERh2AIc1PD4UeLb5IDM7AbgIOMXdX6tvd/dno7/bgXuBpa1exN2vcfdRdx8dGhpKIdnVm7/QTOPN+6tq8xeahd5jWUdklUUageFBYMTMFpjZLOB0YMroIjNbCvwPakFhV8P2OWa2X3T/IGAZ8HgKaYpVxfkLzTTevL+qNn+hWeg9lnlEVhkkDgzu/gbwWeAu4AngZnd/zMwuNbP6KKPLgd8Cbmkalvo+YMLMHgbuAda6+0ACQxXnLzTTePP+qeL8hVZC71X5K79mpPEk7n4ncGfTti803D+hzXk/ARankYZuVXX+QrO48eZVKNn2Q1XnLzQLvVflr/yq5MznuJJKlTKrOgj7o+odz3XKX8VUycBQ9ouyd0PzGfqj6gMb6pS/iqmSgaHsF2XvRtx8BrUDd08DG96i/FVMlQwM7TrEynJR9m6Fqvsattq90GdWpWakOuWv4qlkYGjXIVadLsGpQtV9tQN3L/SZVakZqU75q3gqFxg0jHBfoVpSNT+R3sWtv1XFGqmGRRdP5QKDhhF2R59Id6q8/laIluEulsoFBg0j7J5KdJ1T/mpNw1aLpXKBQcMIW9O6Semo+vpI7WjdpGKpVGDQMML2tG5SOqq+PlI7WjepWCoVGLQ+UnvqIExOAxvCtG5ScVQqMGh9pDB1ECajgQ1hcesmSX5UKjCEVLmaX6cOwmTU8Rym/FUcCgxorH6d1rVJRgMbwpS/iqMygSHUhqlKfo3WtemdBjbEU/4qjlQCg5mtMLMnzWybma1usX8/M7sp2v+Amc1v2HdhtP1JMzsxjfS0oo7nzoQ+i0vWt/8Mq075qzPKX8WQODCY2XTgauAkYBFwhpktajrsbOAFd38PcCXwlejcRdQuBfp+YAXw36LnS506njsT+ixefLX9Z1h1yl+dUf4qhjRqDEcD29x9u7vvAW4EVjYdsxK4Prp/K3C8mVm0/UZ3f83dfw5si54vVbowT+f0WXRP+atzcZ+FmpPyIY3AMAw80/B4R7St5THRNaJ/Dbyjw3MT04V5uhPqRJV9KX91R7Psuze+aSfL1v6IBavvYNnaH/U9gKYRGFr9jDR3w7U7ppNza09gdq6ZTZjZxOTkZFcJfFYX5ulKqBNVJbp96cJP3dEs++6Mb9rJ+bc+zM4XX8WpfUbn3/pwX7+LaQSGHcBhDY8PBZ5td4yZzQD+FbC7w3MBcPdr3H3U3UeHhoa6SuA723R4Hbj/TFXzW1AHYXdCNSzlr31pmffufOl7j/H6m1NLa6+/6cEBD0mlERgeBEbMbIGZzaLWmby+6Zj1wKro/ieAH7m7R9tPj0YtLQBGgP+TQpqmOP/EI9h/5tQ+7f1nTldprg11EHYnVMOS7uij3Fe7gQ2hAQ9JJQ4MUZ/BZ4G7gCeAm939MTO71MxOiQ77GvAOM9sGnAesjs59DLgZeBz4B+BP3f3NpGlqNrZ0mC9/bDHDB+6PUSsRf/lji1Waa0MdhJ3TZ5E+faZvyeqzmJHGk7j7ncCdTdu+0HD/X4BT25y7BliTRjpCxpYOKxB0Yc7bZ7YtkVx+15P6LCPqeO6N8ldnsspflZn5LN1RB2Fn1PHcG+WvzmSVvxQYpKXQMtxaQvot7T6jql7fuVOh/KXc9Zas8pcCg7TVrlNVS0jXhNZH0icUL/TZqZ8h2/ylwCBt6cIqYVofKRkNiw7LMn8pMEhburBKmNZHSkbDosOyzF8KDNKWLqzSntZHSk6fUe/6/dkpMEhburBKexqmmg6ty9W9QXxkCgzSli6s0p6GqaZD63K1lvWFxRQYJEgdhK1pfaR0KH+1lvXABgUGCVIHYWtaHykdyl+tZT2wQYFBgrRu0r6q+J77RflrX3kY2KDAILF0YZWp1PGcLuWvqfKQvxQYJJbWtZlKHc/pUv6aKg/5S4FBYunCKm8JVfO1PlJvQusmQfWak/IwsEGBQRKpWh9sqJpftc8iTaHO/Ko1J+VhYIMCgyRWpRJdqJqv9ZF6p1n2NXn5LiUKDGY218w2mNnW6O+cFscsMbP7zewxM3vEzE5r2PdNM/u5mW2ObkuSpEf6Rx2ENaGFBbU+Uu9Cn12VlnnPQ8czJK8xrAbudvcR4O7ocbNXgD929/cDK4CrzOzAhv3nu/uS6LY5YXqkT9RBWBNaWFD9C70LfXZVWuY9Dx3PkDwwrASuj+5fD4w1H+DuP3P3rdH9Z4FdwFDC15UBUwdh+D1WqVTbL1Vf5j1PAxuSBoaD3f05gOjvvNDBZnY0MAt4qmHzmqiJ6Uoz2y9w7rlmNmFmE5OTkwmTLb2oegdh6D1WqVTbL1Vf5j1PAxtiA4OZ/dDMHm1xW9nNC5nZIcD/BP6du++NNl8IvBf4PWAucEG78939GncfdffRoSFVOLJQ9Q5CdTz3l/JXfvJXbGBw9xPc/cgWt9uB56Mf/PoP/65Wz2Fmvw3cAVzs7hsbnvs5r3kN+AZwdBpvSvqj6stwh5rS1PGcnPJX+32Dzl9Jm5LWA6ui+6uA25sPMLNZwHeBb7n7LU376kHFqPVPPJowPdJHVV6GO3T9XVDHcxqUv9rvH3T+ShoY1gLLzWwrsDx6jJmNmtl10TGfBH4f+HSLYanrzGwLsAU4CLgsYXqkz6q6THLWyyBXhfLXvrLIX4kCg7v/yt2Pd/eR6O/uaPuEu38mun+Du89sGJL6m2Gp7v4hd18cNU2d5e7/nPwtST9VdZnkrJdBrgrlr31lkb8081m6UsUmkzwsg1wVWoZ7X1nkLwUG6VrV5jOEqvlaZjt9oVn2ZWxOipu/kAUFBula1eYzhKr5WmY7faFZ9mVsTsrT/IU6BQbpWtXHmzdSM1L6qvaZ5mn+Qp0Cg3St6uPN66r0Xgct1FxZNnmav1CnwCBdq9J489B70SIY/RNqrixb/srT/IU6BQbpSVXGm+dtfHlVKH9lm78UGKQnVRlvnrfx5VWh/JVt/lJgkJ5UoYNQ8xeyU4X5DHnOXwoM0rOyz2fQ/IVslX0+Q57zlwKD9Kzs8xk0fyFbZZ/PkOf8pcAgPSvzfIY8V/OrogrNSe1knb8UGKRncZ1jRf7i5rmaXyVlbU7K4zIYjRQYpGdxpZoiNyfluZpfJWVtTsrjMhiNFBgkkTI3J7WTdTW/Ssr6WedxGYxGiQKDmc01sw1mtjX6O6fNcW82XKRnfcP2BWb2QHT+TdHV3qRAyticlPdqftWUbfRbXJrzMD8maY1hNXC3u48Ad0ePW3m14SI9pzRs/wpwZXT+C8DZCdMjA1bG5qS8V/Orpmyj3+LSnIdaUtLAsBK4Prp/PbXrNnckus7zh4Bbezlf8qNszUl5r+ZXjfLX4CUNDAe7+3MA0d95bY57m5lNmNlGM6v/+L8DeNHd34ge7wCyD5XStTKttlqEan7VlK25Mo+rqTaLDQxm9kMze7TFbWUXr3O4u48CnwKuMrOFtP7NaFtpNLNzo+AyMTk52cVLS7/FrbZaJEWo5ldN2Zor87iaarPYwODuJ7j7kS1utwPPm9khANHfXW2e49no73bgXmAp8EvgQDObER12KPBsIB3XuPuou48ODQ118RYla0Uq0RWhml9FZWlOKsp3IWlT0npgVXR/FXB78wFmNsfM9ovuHwQsAx53dwfuAT4ROl+KoQwTkdSMlF9laa4sysTJpIFhLbDczLYCy6PHmNmomV0XHfM+YMLMHqYWCNa6++PRvguA88xsG7U+h68lTI9kpAwTkdSMlF9laa4sysTJRIHB3X/l7se7+0j0d3e0fcLdPxPd/4m7L3b3o6K/X2s4f7u7H+3u73H3U939tWRvR7JShnVt1IxUXEXIX0Vaf0sznyU1oeakC297ZIAp6U2oSULNSNkrev666Ltb2u7LUzMSKDBIikLNSa++vjfXpbrxTTuDTRJ5Ks1VVdHz18t73my7P0/NSKDAICkq8rDCPKdNasqcv/JW8FBgkFSFqvt5HlYYSlveqvlVpvw1GAoMkqpQdT+v4pog8lbNr7K4/JXX5qRQ/1Ue85cCg6SqiKOTQmPLIX/V/CqL+1/kcc5MEfuvFBhkoPLYDhwaW65hqvkT+p/kcc5MHvN8HAUGSV1R24Fb0TDV/Cna/6Ro/QugwCB9UKR24CJNOpKasaXDwTb7IuWvPPYvgAKD9EGR2oHj+hckn0Jt9kXKX3kteCgwSF8UpR1Y/QvFpPzVXwoM0hdFuLiKVlMtLuWv/lJgkL4YWzrM7FnT2+7PQ3U/tHbN/jOn5baaL/H5Kw9rJ4WakfKevxQYpG/WfHRx231ZV/fj1q758sc+MMDUSC9C+SsPayeFmpHynr8UGKRv8jzZraidgvKWPA9yKPpoNwUGyUyWX9xQaS6vY8tlX6E5M1nWSos+2i1RYDCzuWa2wcy2Rn/ntDjmD8xsc8PtX8xsLNr3TTP7ecO+JUnSI/mT1y9uSF7Hlsu+8ro2V9ELHklrDKuBu919BLg7ejyFu9/j7kvcfQnwIeAV4AcNh5xf3+/umxOmR3Imj5Pdil7Nl7fksbmyqJPaGiUNDCuB66P71wNjMcd/Avi+u7+S8HWlIPI4OqlIV9KSeHm7slsZRrslDQwHu/tzANHfeTHHnw58u2nbGjN7xMyuNLP92p1oZuea2YSZTUxOTiZLtQxUnkYnFe1KWhIvT1d2K8tot9jAYGY/NLNHW9xWdvNCZnYIsBi4q2HzhcB7gd8D5gIXtDvf3a9x91F3Hx0aGurmpSVjearuh0pzoGakIor7nw2y1lCW0W6xgcHdT3D3I1vcbgeej37w6z/8uwJP9Ungu+7+myKiuz/nNa8B3wCOTvZ2pIgG9cWNK82pGam4Qs1Jg6w1FL3TuS5pU9J6YFV0fxVwe+DYM2hqRmoIKkatf+LRhOmRnMrDFzeuNKdmpOKKG+QwiL6sMnQ61yUNDGuB5Wa2FVgePcbMRs3suvpBZjYfOAz4303nrzOzLcAW4CDgsoTpkZyK++IOotYQKs0VpVNQWosb5DCIvqwyNVMmCgzu/it3P97dR6K/u6PtE+7+mYbjfuHuw+6+t+n8D7n74qhp6ix3/+ck6ZH8ivvi9rvWEPfcRekUlPZCgxygv31ZZWum1MxnGZi4L24/q/tlKs1Ja1l2QpetmVKBQQYmq+p+2Upz0l5WfVlla6ZUYJCByqK6H1dbKFppTtrLohO6jM2UCgwyUHElp/NvSXdVlLjaQhFLc9JeFrXSC74TbqIqYv5SYJCBC1X3X98LZ157f2qvFfelLWJpTsLiaqVp5q8zr72f197Y23Z/UZspFRhk4OKq+/c9tTuVJqXxTTuDX1rVFsop7n+aZv6676ndwWOK2kypwCADF1fdh3RGkKi2UF2hWimk02QZ13dV5IKHAoNkIq66n3QEycXjW4K1BShm2690Jq5W+vreWh7pVVzfFRS74KHAIJkYWzrMsoVzg8ckKdXdsPHp4P6zjj285+eW/Oskf8XlkZC42uiyhXMLXfBQYJDMrDvnuOD+Xjui486ZOQ0uGwvXWKT41p1zHPvNCP/E9Zq/4mqjcXk77xQYJFNxbcHddhRePL4ltkPw8lN1Bdmq+MrHw8059z21u6smpU46nIs6EqmRAoNkqpNr9p53U2dNSuObdsY2D8ycpr6FKhlbOhxba7hh49MdFz7+4paHY48p6kikRgoMkqmxpcOx7f17geVX3Bv7XJ18aVVbqJ64WgN0VvhYfsW9vLHXg8ecdezhpSh4KDBI5i4bWxzbUbh118ttg8P4pp28e/UdsV/aoncISm866YjeCxyzZkPb/ces2cDWXS8Hn2PZwrml6btSYJBc6KSzbuuul3nPf7lzSrX/4vEtfP6mzYS7AmtNSEXvEJTeddIR/fxLe3j3hXdMyV/jm3Yyf/UdPP/Sno5eoyzMPVzKCp5sdipwCfA+4Gh3n2hz3Argb4DpwHXuXr+gzwLgRmrXe/4p8EfuHvsfGB0d9YmJli8lBXbx+JZEQwhDrjptiWoLFTe+aSef77C/qltnHXt4IWoLZvaQu4/GHZe0xvAo8DHgx4GETAeuBk4CFgFnmNmiaPdXgCvdfQR4ATg7YXqkwC4bW8zIvNmpP6+akAQ6a1Lqxci82YUICt1IegW3J9z9yZjDjga2ufv2qDaLANEBAAAFmklEQVRwI7Ayus7zh4Bbo+Oup3bdZ6mwDed9kBnTLLXnG5k3u1RVfElm3TnHpVr4GJk3mw3nfTC158uLQfQxDAPPNDzeEW17B/Ciu7/RtF0q7qunHpXK85T1SyvJbDjvgxx8wKzEz3PwAbNKm79iA4OZ/dDMHm1xW9nha7Qq/nlge7t0nGtmE2Y2MTk52eFLSxGNLR3mqtOWJCq1KChIyAMXLU8UHA4+YBYPXLQ8xRTlS+x3z91PcPcjW9xu7/A1dgCHNTw+FHgW+CVwoJnNaNreLh3XuPuou48ODQ11+NJSVGNLh9m+9uSeqv3LFs5VUJBYD1y0vKc+h5F5s0sdFGAwTUkPAiNmtsDMZgGnA+u9NhzqHuAT0XGrgE6DjVTEhvM+2PGCd/vNmMZVpy1Rn4J0bN05x3HVaUuY2cEvoVEbfVSFQkfS4aofBf4WGAJeBDa7+4lm9k5qw1I/Eh33EeAqasNVv+7ua6Lt7+at4aqbgLPc/bW419Vw1eo689r791mrpihDBSX/Wg2ZXrZwbmkKG50OV00UGLKiwCAi0r1BzWMQEZGSUWAQEZEpFBhERGQKBQYREZlCgUFERKZQYBARkSkKOVzVzCaBf+rx9IOozbousqK/h6KnH4r/HpT+7GXxHt7l7rFLRxQyMCRhZhOdjOPNs6K/h6KnH4r/HpT+7OX5PagpSUREplBgEBGRKaoYGK7JOgEpKPp7KHr6ofjvQenPXm7fQ+X6GEREJKyKNQYREQmoVGAwsxVm9qSZbTOz1VmnpxtmdpiZ3WNmT5jZY2b2Z1mnqRdmNt3MNpnZ/8o6Lb0wswPN7FYz+8fof1G49ZjN7M+jPPSomX3bzN6WdZpCzOzrZrbLzB5t2DbXzDaY2dbo75ws0xinzXu4PMpHj5jZd83swCzT2KgygcHMpgNXAycBi4AzzGxRtqnqyhvAf3L39wHHAn9asPTX/RnwRNaJSOBvgH9w9/cCR1Gw92Jmw8DngFF3P5LaNVJOzzZVsb4JrGjathq4291HgLujx3n2TfZ9DxuAI939A8DPgAsHnah2KhMYgKOBbe6+3d33ULtAUKfXrc6cuz/n7j+N7r9E7QdpONtUdcfMDgVOBq7LOi29MLPfBn4f+BqAu+9x9xezTVVPZgD7R5fVfTuBS+rmgbv/GNjdtHklcH10/3pgbKCJ6lKr9+DuP3D3N6KHG6ld3jgXqhQYhoFnGh7voGA/rHVmNh9YCjyQbUq6dhXwn4G9WSekR+8GJoFvRM1h15lZ9xelzpC77wS+CjwNPAf82t1/kG2qenKwuz8HtUITMC/j9CT174HvZ52IuioFBmuxrXBDsszst4DvAJ939/+XdXo6ZWZ/COxy94eyTksCM4DfBf7e3ZcCL5P/Jowporb4lcAC4J3AbDM7K9tUVZuZXUStqXhd1mmpq1Jg2AEc1vD4UHJehW5mZjOpBYV17n5b1unp0jLgFDP7BbVmvA+Z2Q3ZJqlrO4Ad7l6vqd1KLVAUyQnAz9190t1fB24D/k3GaerF82Z2CED0d1fG6emJma0C/hA403M0d6BKgeFBYMTMFpjZLGodbuszTlPHzMyotW0/4e5XZJ2ebrn7he5+qLvPp/bZ/8jdC1VSdff/CzxjZkdEm44HHs8wSb14GjjWzN4e5anjKVgHemQ9sCq6vwq4PcO09MTMVgAXAKe4+ytZp6dRZQJD1MnzWeAual+Em939sWxT1ZVlwB9RK2lvjm4fyTpRFfQfgXVm9giwBPirjNPTlai2cyvwU2ALtd+A3M7ABTCzbwP3A0eY2Q4zOxtYCyw3s63A8uhxbrV5D38HHABsiL7P/z3TRDbQzGcREZmiMjUGERHpjAKDiIhMocAgIiJTKDCIiMgUCgwiIjKFAoOIiEyhwCAiIlMoMIiIyBT/HykX4cSa8Ko8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(0,720, 1) * 2*np.pi / 360\n",
    "sinx = np.sin(x)\n",
    "plt.scatter(x, sinx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorPairs = []\n",
    "pairs = []\n",
    "for i in range(5, len(sinx) - 5):\n",
    "    p1 = torch.tensor(sinx[i-5: i], dtype=torch.long, device=device).view(-1, 1)\n",
    "    p2 = torch.tensor(sinx[i: i+5], dtype=torch.long, device=device).view(-1, 1)\n",
    "    tensorPairs.append((p1, p2))\n",
    "    pairs.append((sinx[i-5: i], sinx[i: i+5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    SOS_token = 0\n",
    "    EOS_token = 1\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[0]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di].long())\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di].long())\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [random.choice(tensorPairs)\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[0]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of type torch.FloatTensor but found type torch.LongTensor for argument #4 'mat1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-b0e110038cf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mattn_decoder1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttnDecoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumFeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_decoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m75000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-f961ac69a9dc>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, n_iters, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         loss = train(input_tensor, target_tensor, encoder,\n\u001b[0;32m---> 19\u001b[0;31m                      decoder, encoder_optimizer, decoder_optimizer, criterion)\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mplot_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-ddb5c98efd31>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             decoder_output, decoder_hidden, decoder_attention = decoder(\n\u001b[0;32m---> 31\u001b[0;31m                 decoder_input, decoder_hidden, encoder_outputs)\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdi\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Teacher forcing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-55c4834220c0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         attn_weights = F.softmax(\n\u001b[0;32m---> 24\u001b[0;31m             self.attn(torch.cat((embedded[0].long(), hidden[0].long()), 1)), dim=1)\n\u001b[0m\u001b[1;32m     25\u001b[0m         attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n\u001b[1;32m     26\u001b[0m                                  encoder_outputs.unsqueeze(0))\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of type torch.FloatTensor but found type torch.LongTensor for argument #4 'mat1'"
     ]
    }
   ],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "hidden_size = 10\n",
    "numFeatures = 1\n",
    "encoder1 = EncoderRNN(numFeatures, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, numFeatures, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 75000, print_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
